import torch

# 假设 logit_diff_per_layer 是一个包含每一层 logit 差异的张量
logit_diff_per_layer = torch.tensor([[ 2.0628e-03, -2.3460e-02, -1.2513e-04, -1.4532e-04,  6.4619e-04,
          7.4359e-04, -2.5292e-03,  2.4175e-03],
        [ 1.5884e-02,  1.1386e-02, -2.1240e-03, -7.9223e-04,  8.9147e-03,
         -7.1834e-03, -3.1531e-03,  8.0004e-03],
        [-2.3803e-02,  1.0146e-02,  1.2438e-02,  1.7864e-02,  9.3093e-03,
          3.0934e-04, -1.2389e-02, -7.2467e-03],
        [-3.2452e-03, -3.5452e-02,  1.4388e-02,  2.4837e-02,  6.0279e-03,
         -3.3313e-02, -4.2188e-02,  8.6191e-03],
        [ 3.0522e-03, -5.7600e-03,  3.8365e-03,  1.0450e-02,  1.8813e-02,
         -2.2933e-02,  1.3695e-02, -2.0093e-02],
        [-2.9165e-02,  1.7528e-02,  1.7579e-02, -1.2907e-02, -1.9422e-02,
          3.0438e-02,  4.1803e-02,  3.2388e-02],
        [ 2.6865e-02,  2.4663e-02, -5.5055e-02,  1.5452e-02,  1.6449e-03,
          1.8632e-02, -4.2045e-02, -5.3288e-03],
        [ 4.6621e-02, -1.0377e-02, -2.0696e-02, -2.4009e-02,  5.1235e-02,
         -2.2818e-02,  4.6421e-02,  8.6992e-02],
        [ 2.0771e-02,  6.7519e-02, -1.4676e-02, -1.3256e-02, -1.6443e-02,
          4.9522e-02, -4.3682e-02, -3.9982e-02],
        [ 2.5595e-02, -6.9687e-02,  2.7819e-02,  6.5588e-02,  3.6297e-02,
         -1.2815e-02,  1.5382e-02, -2.9653e-02],
        [-1.0526e-01, -4.5308e-02, -1.7724e-02,  9.3394e-03, -5.0170e-02,
         -7.4935e-03, -9.6622e-03, -1.6295e-02],
        [ 1.2041e-02, -1.7474e-02,  3.2310e-02, -1.4200e-03,  1.1689e-02,
         -4.7196e-02,  5.0567e-02, -1.8959e-02],
        [-5.1720e-02,  1.9971e-01, -2.3421e-02, -2.2806e-02, -2.6505e-02,
          7.8921e-02, -3.2149e-02, -3.4268e-02],
        [-1.7091e-02,  6.7276e-02,  2.9896e-02, -3.8904e-02, -5.4039e-02,
          1.1484e-02,  1.6332e-02,  2.1518e-02],
        [ 9.9096e-02, -2.1969e-01, -5.1117e-02,  5.5283e-02,  8.6115e-02,
         -5.8180e-02,  7.5654e-02, -1.1045e-01],
        [-5.7751e-02,  7.5117e-02,  6.1403e-03, -1.3631e-02, -5.3857e-03,
         -3.2328e-03,  1.0041e-01,  7.5815e-02],
        [ 6.5689e-02, -8.0166e-02,  1.0554e-01, -4.0380e-02, -7.4450e-02,
          1.0490e-01,  5.0802e-02,  2.0297e-03],
        [-3.4423e-02,  3.4837e-02,  2.7178e-02,  2.1638e-02, -5.9684e-02,
         -3.3152e-02,  8.6130e-02,  4.5081e-03],
        [-3.3072e-02, -5.8287e-02, -6.0172e-02,  1.2117e-01, -9.3730e-03,
          1.1189e-01,  7.1164e-02, -1.7972e-02],
        [ 3.3850e-02,  1.6673e-01, -6.6475e-03, -9.6034e-02,  6.6883e-02,
          1.0285e-02,  4.7407e-02,  3.3469e-02],
        [-6.2976e-02, -3.0786e-02, -2.9686e-02, -1.4252e-02,  1.4360e-01,
         -8.9609e-02, -1.8117e-01,  3.5601e-02],
        [ 1.0642e-01,  1.5649e-01,  9.2124e-02, -1.7992e-02, -2.4562e-02,
         -3.9633e-02,  4.1940e-02,  2.0385e-03],
        [-6.9456e-02,  4.6905e-03, -1.2398e-01,  2.0226e-01,  7.1628e-03,
         -3.7413e-02, -7.9743e-02, -1.6251e-01],
        [ 2.9088e-02, -2.4165e-02,  7.3098e-02,  2.5335e-02, -1.3935e-01,
          2.6507e-02,  1.9213e-02, -2.7959e-02],
        [ 7.2843e-03, -1.2929e-01,  9.4388e-02, -3.0038e-02,  1.2656e-04,
         -5.2117e-02,  5.1023e-02,  4.8232e-02],
        [-1.0711e-01, -1.1740e-01,  1.5225e-01, -2.5096e-02, -5.6070e-02,
          2.0266e-01,  1.2811e-01,  7.0190e-02],
        [ 1.8342e-02, -1.5267e-01, -1.6911e-01, -2.3530e-02,  7.4395e-02,
          2.0225e-01, -9.2518e-02, -1.6345e-01],
        [-6.4506e-02, -2.3049e-02, -7.0869e-03,  5.6640e-02, -3.2679e-02,
         -4.5738e-02, -3.2925e-02, -7.9053e-02],
        [-9.3630e-02,  2.3510e-02, -1.1613e-02,  9.0285e-03,  1.7480e-01,
          2.6187e-01,  1.8538e-02,  7.5998e-02],
        [-1.0673e-01,  8.0907e-02, -1.4526e-02, -1.3460e-01, -1.3969e-01,
          4.8929e-02,  7.3308e-02, -3.6144e-02],
        [-1.6953e-01,  6.5677e-02,  6.6939e-02,  1.6275e-01,  1.1689e-01,
         -3.8285e-02,  1.8324e-01,  7.9945e-02],
        [ 1.9272e-02,  2.6235e-01,  9.6824e-02, -2.7341e-02,  6.4362e-03,
         -1.3939e-01,  1.0091e-01, -1.2426e-01],
        [ 3.1275e-02, -1.2709e-01,  7.7857e-02, -7.1057e-02,  1.1819e-01,
         -1.7281e-01, -1.9076e-01,  3.2851e-02],
        [-7.3820e-02,  2.7887e-02, -6.6035e-02, -2.2430e-02, -1.1508e-01,
         -4.2341e-02,  6.6562e-02, -1.8195e-02],
        [-1.3864e-01, -3.6791e-03, -1.0762e-02, -9.6722e-02,  3.5614e-01,
          2.5102e-02, -1.1998e-01,  6.9548e-02],
        [-7.7445e-02, -5.9685e-03,  4.8108e-02,  2.6061e-02, -4.8017e-02,
         -8.7167e-03,  9.0194e-03, -6.2384e-02],
        [-6.4184e-02, -7.3547e-02,  1.9576e-01, -6.2909e-02, -2.6832e-01,
         -1.2846e-02,  2.3345e-01, -1.3134e-02],
        [ 2.0266e-01, -1.8499e-03, -3.0604e-03, -1.3957e-01,  7.4985e-03,
          1.0445e-01, -4.3065e-02, -7.9522e-02],
        [-1.2044e-01, -1.6637e-01, -1.0803e-01, -1.4692e-01,  2.0027e-01,
          1.7988e-01, -3.4251e-01, -1.3838e-01],
        [-1.5667e-01, -1.3377e-01, -4.5349e-03,  3.3115e-02,  2.2728e-02,
         -1.4537e-01, -2.8004e-03,  2.7030e-02],
        [-2.3038e-02, -3.7904e-01, -1.3101e-01, -1.5595e-01,  1.5046e-01,
         -2.2943e-01, -8.7467e-02,  8.3828e-02],
        [ 1.3881e-01,  1.1895e-01,  1.4858e-01, -1.0888e-01,  5.0020e-02,
          2.3568e-02, -1.7694e-01, -2.2382e-04],
        [-5.7048e-03, -2.5900e-01, -1.5400e-01,  6.1885e-02, -1.2775e-01,
         -5.7848e-02, -7.4648e-02,  1.0915e-01],
        [ 7.6618e-02, -6.2970e-01, -1.3564e-01,  1.6815e-02,  8.0201e-03,
          5.1883e-02, -1.5317e-02,  5.7299e-02],
        [-4.4262e-02,  3.6727e-02, -2.9837e-02,  2.2711e-01,  2.2095e-02,
          1.1033e-02,  1.5198e-01, -2.7511e-01],
        [ 3.6299e-02, -1.2302e+00,  2.4325e-02,  5.8298e-03,  1.4225e-03,
         -8.6996e-03,  1.2091e-02,  1.1073e-02],
        [ 1.4597e-01, -9.0465e-01,  1.2854e-01, -1.1760e-01, -1.7270e-01,
         -1.8883e-01,  1.8326e-01, -9.6480e-02],
        [-3.4912e-04,  1.0376e-01,  1.0404e-02, -9.4434e-03,  2.1202e-01,
          3.9342e-01, -1.8236e-02,  4.5307e-02],
        [-1.2023e-01,  5.1925e-01, -6.2004e-02, -1.0707e-01,  8.9249e-02,
         -1.5698e-01, -2.2577e-01, -2.6887e-02],
        [-3.3919e-01,  2.2620e+00, -6.3750e-02,  4.9814e-02,  3.7786e-01,
         -6.1357e-02,  3.8812e-01,  2.3966e-03],
        [-2.4258e-01, -4.4958e-01,  1.9773e-02,  9.6877e-02, -7.3046e-02,
          1.3958e-01, -1.0254e-01,  1.2247e-01],
        [ 6.2764e-03, -2.0217e+00,  5.5729e-02,  7.9446e-04,  1.4550e-01,
          5.8173e-01, -5.7111e-02,  1.3001e-01],
        [-4.8719e-01,  4.5463e-01,  4.1466e-02,  2.9893e-02, -3.1149e-02,
         -5.9624e-02, -1.4295e-01,  8.3515e-02],
        [-5.1160e-01,  6.9247e-01, -1.6629e-01,  1.8191e-01, -2.0592e-01,
         -1.6724e-01, -4.8091e-02,  2.6501e-01],
        [ 1.2281e-02, -1.7592e+00, -1.1305e-02,  3.6707e-01, -2.8257e-01,
          1.8984e-01,  4.0925e-01,  4.3981e-02],
        [-3.0681e-02,  2.3566e-01, -2.7292e-01, -1.8205e-01,  9.5901e-02,
          8.9546e-02, -3.3723e-01,  1.1921e-01],
        [ 2.0938e-01, -8.2680e-01, -6.9521e-02,  1.2613e-01, -4.3683e-02,
          3.7368e-02, -1.1774e-01, -8.0137e-03],
        [ 1.1131e-01, -9.5207e-01, -1.5045e-01, -6.5662e-02, -1.9661e-02,
          4.2070e-01, -3.7097e-02,  1.5015e-01],
        [ 6.7464e-01,  2.3810e+00, -2.9439e-01,  2.4876e-01,  2.8742e-01,
          1.1480e-01,  3.5521e-01,  8.7513e-02],
        [-4.6468e-02,  6.2703e-01,  1.4436e-01, -1.2783e-01, -5.0700e-02,
          6.7892e-02, -2.3852e-01,  1.3486e-01],
        [-9.7418e-02,  9.2698e-01, -5.9341e-01,  2.1484e-01, -9.0075e-02,
          5.9388e-01, -4.5219e-01, -3.2070e-01],
        [ 6.0137e-01,  1.0998e+00,  2.5734e-01, -1.8590e-01, -5.9252e-02,
         -1.9848e-01,  6.2730e-02, -3.5598e-01],
        [ 1.7266e+00, -1.7452e+00, -3.0123e-02, -4.8111e-01, -1.7351e-01,
          4.2976e-01,  4.2977e-02, -2.4388e-02],
        [-1.4439e-01,  8.7810e-01, -6.6341e-02,  7.1574e-01, -2.8495e-01,
          7.3437e-01,  2.3315e-01,  1.6979e-01],
        [-3.4392e+00,  1.2220e+00, -2.1392e-01, -2.2974e-01, -1.2703e+00,
         -2.0254e+00, -6.5263e-01,  4.0349e-01]])

# 计算每层的均值
mean_values = logit_diff_per_layer.mean(dim=1)

# 将均值与对应层数一起存储，并按均值排序
sorted_means = sorted(enumerate(mean_values), key=lambda x: x[1], reverse=True)

# 打印排序结果
for idx, mean in sorted_means:
    print(f"Layer {idx+1} - Mean: {mean.item():.4f}")
